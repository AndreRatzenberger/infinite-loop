---
name: LiteLLM
status: Watch
link: https://litellm.ai
why: Swap providers, fallback chains, cost tracking
---

Your app calls OpenAI. Then you need Anthropic. Then Azure. Then a local model. Suddenly you're maintaining four different API integrations. LiteLLM unifies 100+ LLM providers behind OpenAI's API format. Swap providers with a config change, not a code rewrite. Built-in retry/fallback across deployments, budget limits per project, and cost tracking so you know which model is burning money. The proxy server hits 8ms P95 latency at 1k RPS. Born from Y Combinator, now the universal remote for LLM APIs. Stop writing provider-specific code; let LiteLLM handle the chaos.
